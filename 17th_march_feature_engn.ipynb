{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAYJsflkYvOCiuv0t1f7yg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanchandhole/Data-Scientist/blob/main/17th_march_feature_engn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1:** What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
        "algorithms that are not affected by missing values.\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "Here‚Äôs a **clear, well-structured answer** for **Q1** üëá\n",
        "\n",
        "---\n",
        "\n",
        "## **Missing Values in a Dataset**\n",
        "\n",
        "### **What are Missing Values?**\n",
        "\n",
        "Missing values occur when **no data is recorded for a variable** in one or more observations.\n",
        "They may appear as **NaN, NULL, empty cells, or special symbols** in a dataset.\n",
        "\n",
        "**Common Causes:**\n",
        "\n",
        "* Data entry errors\n",
        "* Sensor or system failure\n",
        "* Survey non-responses\n",
        "* Data corruption or loss\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Is It Essential to Handle Missing Values?**\n",
        "\n",
        "Handling missing values is important because:\n",
        "\n",
        "1. **Model Accuracy**\n",
        "   Missing values can lead to incorrect predictions or biased results.\n",
        "\n",
        "2. **Algorithm Limitations**\n",
        "   Many machine learning algorithms cannot work with missing values directly.\n",
        "\n",
        "3. **Statistical Validity**\n",
        "   Ignoring missing data can distort distributions and relationships.\n",
        "\n",
        "4. **Data Consistency**\n",
        "   Clean data ensures reliable analysis and decision-making.\n",
        "\n",
        "---\n",
        "\n",
        "### **Algorithms Not Affected by Missing Values**\n",
        "\n",
        "Some algorithms can **handle missing values internally** or are **less sensitive** to them:\n",
        "\n",
        "* **Decision Trees**\n",
        "* **Random Forest**\n",
        "* **Gradient Boosting (XGBoost, LightGBM, CatBoost)**\n",
        "* **K-Nearest Neighbors (with distance-based handling, depending on implementation)**\n",
        "\n",
        "> Note: Many algorithms still perform better when missing values are properly handled.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "Missing values represent incomplete information in data. Proper handling improves **model performance, reliability, and interpretability**, even when using algorithms that can tolerate missing data.\n"
      ],
      "metadata": {
        "id": "6N7Xiq-ufpIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2:** List down techniques used to handle missing data. Give an example of each with python code.\n",
        "\n",
        "**Ans:**  Techniques to Handle Missing Data (with Python Examples)\n",
        "\n",
        "Let‚Äôs assume we have this sample dataset:"
      ],
      "metadata": {
        "id": "iTjOvGeIf2jI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {\n",
        "    'Age': [25, 30, np.nan, 28, 35],\n",
        "    'Salary': [50000, np.nan, 60000, 58000, np.nan]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4-OlSFFgEqj",
        "outputId": "fe6e4d3c-b1d5-4a70-b91b-5e5fee2b77f6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Age   Salary\n",
            "0  25.0  50000.0\n",
            "1  30.0      NaN\n",
            "2   NaN  60000.0\n",
            "3  28.0  58000.0\n",
            "4  35.0      NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Removing Missing Values (Deletion Method)\n",
        "\n",
        "a) Row-wise deletion\n",
        "\n",
        "Used when missing values are very few."
      ],
      "metadata": {
        "id": "i63YdE6ugLlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_drop_rows = df.dropna()\n",
        "print(df_drop_rows)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GO8wazDgMQH",
        "outputId": "77522456-8bf2-4ac8-dc90-0234afca0853"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Age   Salary\n",
            "0  25.0  50000.0\n",
            "3  28.0  58000.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) Column-wise deletion\n",
        "\n",
        "Used when a column has too many missing values."
      ],
      "metadata": {
        "id": "92jl-KqDgRAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_drop_cols = df.dropna(axis=1)\n",
        "print(df_drop_cols)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sivnmWUmgTrK",
        "outputId": "74dc1770-77bf-48cf-a813-1eb1354b4d69"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: [0, 1, 2, 3, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Mean Imputation\n",
        "\n",
        "Replace missing values with the mean (numerical data)."
      ],
      "metadata": {
        "id": "0h5bHjp4gWdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_mean = df.copy()\n",
        "df_mean['Age'].fillna(df_mean['Age'].mean(), inplace=True)\n",
        "df_mean['Salary'].fillna(df_mean['Salary'].mean(), inplace=True)\n",
        "print(df_mean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BPOr9ZMgYsJ",
        "outputId": "6f12a6cb-4694-4ed5-c40b-b545d172ab48"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Age   Salary\n",
            "0  25.0  50000.0\n",
            "1  30.0  56000.0\n",
            "2  29.5  60000.0\n",
            "3  28.0  58000.0\n",
            "4  35.0  56000.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1897172791.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_mean['Age'].fillna(df_mean['Age'].mean(), inplace=True)\n",
            "/tmp/ipython-input-1897172791.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_mean['Salary'].fillna(df_mean['Salary'].mean(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Median Imputation\n",
        "\n",
        "Used when data has outliers."
      ],
      "metadata": {
        "id": "Py-BgV_QgfWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_median = df.copy()\n",
        "df_median.fillna(df_median.median(), inplace=True)\n",
        "print(df_median)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCd4nd4HgmJY",
        "outputId": "645225a5-a3b2-4eb3-d1d5-1eecfacac695"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Age   Salary\n",
            "0  25.0  50000.0\n",
            "1  30.0  58000.0\n",
            "2  29.0  60000.0\n",
            "3  28.0  58000.0\n",
            "4  35.0  58000.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Mode Imputation\n",
        "\n",
        "Used for categorical data."
      ],
      "metadata": {
        "id": "04g_sIL8gqFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cat = pd.DataFrame({\n",
        "    'City': ['Pune', 'Mumbai', np.nan, 'Pune']\n",
        "})\n",
        "\n",
        "df_cat['City'].fillna(df_cat['City'].mode()[0], inplace=True)\n",
        "print(df_cat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rh60hPSzgsIT",
        "outputId": "3376b7fc-20e6-4cb8-bd6e-85456c258765"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     City\n",
            "0    Pune\n",
            "1  Mumbai\n",
            "2    Pune\n",
            "3    Pune\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1724130849.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_cat['City'].fillna(df_cat['City'].mode()[0], inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Forward Fill (Propagation Method)\n",
        "df_ffill = df.fillna(method='ffill')\n",
        "print(df_ffill)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1R1o08GgxPQ",
        "outputId": "b9f48d0a-3c13-47fa-ebce-8bfd642da9f8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Age   Salary\n",
            "0  25.0  50000.0\n",
            "1  30.0  50000.0\n",
            "2  30.0  60000.0\n",
            "3  28.0  58000.0\n",
            "4  35.0  58000.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2635045136.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df_ffill = df.fillna(method='ffill')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Backward Fill\n",
        "\n",
        "df_bfill = df.fillna(method='bfill')\n",
        "print(df_bfill)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmWXhCWag0SE",
        "outputId": "c3e9f778-9c57-468d-b815-4f9c8945808a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Age   Salary\n",
            "0  25.0  50000.0\n",
            "1  30.0  60000.0\n",
            "2  28.0  60000.0\n",
            "3  28.0  58000.0\n",
            "4  35.0      NaN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2073685808.py:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df_bfill = df.fillna(method='bfill')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Constant Value Imputation\n",
        "\n",
        "#Replace missing values with a fixed value (e.g., 0 or ‚ÄúUnknown‚Äù).\n",
        "\n",
        "df_const = df.fillna(0)\n",
        "print(df_const)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xXKgjizg8Qd",
        "outputId": "283378af-9756-4f7b-f67b-b9108c88e7f0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Age   Salary\n",
            "0  25.0  50000.0\n",
            "1  30.0      0.0\n",
            "2   0.0  60000.0\n",
            "3  28.0  58000.0\n",
            "4  35.0      0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Predictive Imputation (Advanced ‚Äì ML Based)\n",
        "\n",
        "#Use models like regression or KNN to predict missing values.\n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "imputer = KNNImputer(n_neighbors=2)\n",
        "df_knn = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
        "print(df_knn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRku8M9PhA2N",
        "outputId": "e91ca860-cee5-4bc7-99d5-671082a43001"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Age   Salary\n",
            "0  25.0  50000.0\n",
            "1  30.0  54000.0\n",
            "2  26.5  60000.0\n",
            "3  28.0  58000.0\n",
            "4  35.0  54000.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Technique               | Best Used When          |\n",
        "| ----------------------- | ----------------------- |\n",
        "| Deletion                | Very few missing values |\n",
        "| Mean                    | Normal distribution     |\n",
        "| Median                  | Outliers present        |\n",
        "| Mode                    | Categorical data        |\n",
        "| Forward / Backward Fill | Time-series data        |\n",
        "| Constant                | Placeholder needed      |\n",
        "| KNN / ML                | Complex patterns        |\n",
        "\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Choosing the right missing data technique depends on data type, amount of missingness, and business context.\n"
      ],
      "metadata": {
        "id": "fk47E7P1gvLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3:** Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "Here‚Äôs a **clear, exam-ready answer** for **Q3** üëá\n",
        "\n",
        "---\n",
        "\n",
        "## **Imbalanced Data in Machine Learning**\n",
        "\n",
        "### **What is Imbalanced Data?**\n",
        "\n",
        "Imbalanced data occurs when the **classes in a target variable are not represented equally**, meaning one class (majority class) has significantly more observations than the other class(es) (minority class).\n",
        "\n",
        "**Example:**\n",
        "\n",
        "* Fraud detection: 98% non-fraud, 2% fraud\n",
        "* Medical diagnosis: 95% healthy, 5% disease\n",
        "\n",
        "---\n",
        "\n",
        "### **What Happens If Imbalanced Data Is Not Handled?**\n",
        "\n",
        "1. **Biased Model Predictions**\n",
        "   The model tends to predict the **majority class**, ignoring the minority class.\n",
        "\n",
        "2. **Misleading Accuracy**\n",
        "   High accuracy may be achieved, but the model fails to detect important minority cases.\n",
        "\n",
        "   * Example: Predicting ‚Äúnon-fraud‚Äù always gives 98% accuracy.\n",
        "\n",
        "3. **Poor Recall and Precision for Minority Class**\n",
        "   Critical cases (fraud, disease, churn) are missed.\n",
        "\n",
        "4. **Business and Real-World Risks**\n",
        "\n",
        "   * Missed fraud ‚Üí financial loss\n",
        "   * Missed disease ‚Üí health risks\n",
        "\n",
        "5. **Unreliable Model Performance**\n",
        "   The model does not generalize well for minority outcomes.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "If imbalanced data is not handled, the model becomes **biased, unreliable, and ineffective** for decision-making. Handling imbalance is essential for fair and meaningful predictions.\n"
      ],
      "metadata": {
        "id": "_EszWikehRY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4:** What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
        "sampling are required.\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "Here‚Äôs a **clear, exam-ready explanation** for **Q4** üëá\n",
        "\n",
        "---\n",
        "\n",
        "## **Up-sampling and Down-sampling in Machine Learning**\n",
        "\n",
        "Up-sampling and down-sampling are techniques used to **handle imbalanced datasets** by adjusting the number of samples in different classes.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Up-sampling**\n",
        "\n",
        "### **Definition**\n",
        "\n",
        "Up-sampling increases the number of samples in the **minority class** to balance it with the majority class.\n",
        "\n",
        "This is usually done by **duplicating existing minority samples** or **creating synthetic samples**.\n",
        "\n",
        "### **When Up-sampling Is Required**\n",
        "\n",
        "* Minority class has **very few samples**\n",
        "* Losing minority data is risky (e.g., fraud, disease detection)\n",
        "\n",
        "### **Example**\n",
        "\n",
        "Fraud detection dataset:\n",
        "\n",
        "* Fraud cases: 200\n",
        "* Non-fraud cases: 9,800\n",
        "\n",
        "Up-sampling increases fraud cases to match non-fraud cases.\n",
        "\n",
        "### **Advantages**\n",
        "\n",
        "* Preserves all original data\n",
        "* Improves detection of minority class\n",
        "\n",
        "### **Disadvantages**\n",
        "\n",
        "* Risk of overfitting due to duplicated data\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Down-sampling**\n",
        "\n",
        "### **Definition**\n",
        "\n",
        "Down-sampling reduces the number of samples in the **majority class** to balance the dataset.\n",
        "\n",
        "This is done by **randomly removing majority class samples**.\n",
        "\n",
        "### **When Down-sampling Is Required**\n",
        "\n",
        "* Dataset is very large\n",
        "* Majority class dominates heavily\n",
        "\n",
        "### **Example**\n",
        "\n",
        "Customer churn dataset:\n",
        "\n",
        "* Non-churn: 50,000\n",
        "* Churn: 5,000\n",
        "\n",
        "Down-sampling reduces non-churn samples to 5,000.\n",
        "\n",
        "### **Advantages**\n",
        "\n",
        "* Faster training\n",
        "* Reduces model bias\n",
        "\n",
        "### **Disadvantages**\n",
        "\n",
        "* Loss of potentially useful data\n",
        "\n",
        "---\n",
        "\n",
        "## **Comparison Table**\n",
        "\n",
        "| Aspect           | Up-sampling    | Down-sampling  |\n",
        "| ---------------- | -------------- | -------------- |\n",
        "| Targets          | Minority class | Majority class |\n",
        "| Data Size        | Increases      | Decreases      |\n",
        "| Data Loss        | No             | Yes            |\n",
        "| Overfitting Risk | Higher         | Lower          |\n",
        "| Best For         | Small datasets | Large datasets |\n",
        "\n",
        "---\n",
        "\n",
        "## **Conclusion**\n",
        "\n",
        "* Use **up-sampling** when minority class data is limited and valuable.\n",
        "* Use **down-sampling** when the dataset is large and majority class dominates.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ndtFyRg0hd9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5:** What is data Augmentation? Explain SMOTE.\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "Here‚Äôs a **clear, exam-ready explanation** for **Q5** üëá\n",
        "\n",
        "---\n",
        "\n",
        "## **Data Augmentation**\n",
        "\n",
        "### **What is Data Augmentation?**\n",
        "\n",
        "Data augmentation is a technique used to **artificially increase the size and diversity of a dataset** by creating new data samples from existing ones. It helps improve model performance and reduce **overfitting**, especially when data is limited or imbalanced.\n",
        "\n",
        "**Common Examples:**\n",
        "\n",
        "* Image data: rotation, flipping, zooming\n",
        "* Text data: synonym replacement\n",
        "* Tabular data: synthetic sample generation (e.g., SMOTE)\n",
        "\n",
        "---\n",
        "\n",
        "## **SMOTE (Synthetic Minority Over-sampling Technique)**\n",
        "\n",
        "### **What is SMOTE?**\n",
        "\n",
        "SMOTE is a **data augmentation technique for imbalanced datasets** that generates **synthetic samples for the minority class** instead of duplicating existing ones.\n",
        "\n",
        "### **How SMOTE Works**\n",
        "\n",
        "1. Select a minority class data point\n",
        "2. Find its **k-nearest neighbors**\n",
        "3. Create a new synthetic point **between the data point and one of its neighbors**\n",
        "\n",
        "This results in more realistic and diverse minority samples.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why SMOTE Is Better Than Simple Up-sampling**\n",
        "\n",
        "* Avoids exact duplication\n",
        "* Reduces overfitting\n",
        "* Produces more generalized samples\n",
        "\n",
        "---\n",
        "\n",
        "### **Example Scenario**\n",
        "\n",
        "In fraud detection:\n",
        "\n",
        "* Fraud cases = 2%\n",
        "* Non-fraud cases = 98%\n",
        "\n",
        "SMOTE generates new fraud samples to balance the dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### **Advantages of SMOTE**\n",
        "\n",
        "* Improves minority class recall\n",
        "* Creates diverse synthetic data\n",
        "* Works well for numeric features\n",
        "\n",
        "---\n",
        "\n",
        "### **Limitations of SMOTE**\n",
        "\n",
        "* Not suitable for categorical data (without modification)\n",
        "* Can create overlapping classes\n",
        "* Sensitive to noise\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "Data augmentation improves model learning by increasing data diversity. **SMOTE** is a powerful technique for handling class imbalance by generating synthetic minority samples, leading to better and fairer predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "LpfmAwX-hrDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6:** What are outliers in a dataset? Why is it essential to handle outliers?\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "Here‚Äôs a **clear, exam-ready answer** for **Q6** üëá\n",
        "\n",
        "---\n",
        "\n",
        "## **Outliers in a Dataset**\n",
        "\n",
        "### **What Are Outliers?**\n",
        "\n",
        "Outliers are **data points that differ significantly from other observations** in a dataset.\n",
        "They can be unusually high or low values compared to the majority of the data.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "* Age = 150 in a human dataset\n",
        "* Salary = $1,000,000 when most are $30,000‚Äì$50,000\n",
        "\n",
        "**Causes of Outliers:**\n",
        "\n",
        "* Data entry errors or typos\n",
        "* Measurement or sensor errors\n",
        "* Natural variability in the population\n",
        "* Fraudulent data\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Is It Essential to Handle Outliers?**\n",
        "\n",
        "1. **Impact on Statistical Measures**\n",
        "\n",
        "   * Outliers can **distort mean, standard deviation, and correlations**.\n",
        "   * Example: A single extremely high salary can inflate the average.\n",
        "\n",
        "2. **Affect Model Performance**\n",
        "\n",
        "   * Many machine learning models (e.g., linear regression, k-NN) are **sensitive to extreme values**.\n",
        "   * Outliers can lead to poor predictions and biased coefficients.\n",
        "\n",
        "3. **Influence on Visualizations**\n",
        "\n",
        "   * Outliers can **mislead plots** like boxplots or scatter plots.\n",
        "\n",
        "4. **Impact on Distance-Based Algorithms**\n",
        "\n",
        "   * Algorithms like **k-NN or clustering** rely on distance metrics; outliers can dominate distances and distort results.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "Handling outliers is crucial to ensure **accurate statistics, robust models, and reliable analysis**. Common approaches include **removal, transformation, or capping (winsorization)**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bz1K-Kqdh5fg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7:** You are working on a project that requires analyzing customer data. However, you notice that some of\n",
        "the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "Here‚Äôs a **clear, exam-ready answer** for **Q7** üëá\n",
        "\n",
        "---\n",
        "\n",
        "## **Handling Missing Data in Customer Analysis**\n",
        "\n",
        "When working with datasets containing missing values, it‚Äôs important to handle them carefully to ensure accurate analysis. Some common techniques include:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Deletion Methods**\n",
        "\n",
        "* **Row-wise deletion:** Remove rows that contain missing values.\n",
        "* **Column-wise deletion:** Remove columns with too many missing values.\n",
        "  **Use case:** When missing data is minimal and removing it won‚Äôt affect the analysis.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Imputation Methods**\n",
        "\n",
        "* **Mean Imputation:** Replace missing numerical values with the column mean.\n",
        "* **Median Imputation:** Replace missing values with the median (useful for skewed data).\n",
        "* **Mode Imputation:** Replace missing categorical values with the mode.\n",
        "* **Constant Value:** Fill missing data with a fixed value, e.g., ‚ÄúUnknown‚Äù or 0.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Forward / Backward Fill**\n",
        "\n",
        "* **Forward fill:** Replace missing value with the previous value in the column (time-series data).\n",
        "* **Backward fill:** Replace missing value with the next value.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Predictive Imputation**\n",
        "\n",
        "* Use machine learning models (e.g., **KNN, regression**) to predict missing values based on other features.\n",
        "  **Use case:** When data has patterns that can help estimate missing values.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Specialized Techniques**\n",
        "\n",
        "* **Multiple Imputation:** Creates multiple imputed datasets and combines results.\n",
        "* **Interpolation:** Estimate missing values based on trends (time-series data).\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "Choosing the right technique depends on:\n",
        "\n",
        "* The **amount of missing data**\n",
        "* **Data type** (numerical or categorical)\n",
        "* **Business context** and importance of the missing values\n",
        "\n",
        "Proper handling ensures **accurate insights** and **robust models**.\n"
      ],
      "metadata": {
        "id": "YMIV9mZ5iK4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8:** You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
        "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
        "to the missing data?\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "\n",
        "Here‚Äôs a **clear, exam-ready answer** for **Q8** üëá\n",
        "\n",
        "---\n",
        "\n",
        "## **Determining Patterns in Missing Data**\n",
        "\n",
        "When a small percentage of data is missing, it is important to check **why the data is missing** to decide how to handle it. Missing data can be:\n",
        "\n",
        "* **MCAR (Missing Completely at Random):** No pattern; missingness is unrelated to any variable.\n",
        "* **MAR (Missing at Random):** Missingness depends on other observed variables.\n",
        "* **MNAR (Missing Not at Random):** Missingness depends on the unobserved value itself.\n",
        "\n",
        "---\n",
        "\n",
        "### **Strategies to Detect Patterns**\n",
        "\n",
        "1. **Visual Inspection**\n",
        "\n",
        "   * Use heatmaps or missing value plots to see the distribution.\n",
        "\n",
        "   ```python\n",
        "   import seaborn as sns\n",
        "   sns.heatmap(df.isnull(), cbar=False)\n",
        "   ```\n",
        "\n",
        "   * Random missing values indicate MCAR; clusters indicate MAR or MNAR.\n",
        "\n",
        "2. **Summary Statistics**\n",
        "\n",
        "   * Compare statistics of rows with missing vs. non-missing data.\n",
        "   * Significant differences may indicate MAR or MNAR.\n",
        "\n",
        "3. **Correlation Analysis**\n",
        "\n",
        "   * Check correlations between missingness and other variables.\n",
        "   * Example: `df['feature'].isnull().astype(int).corr(df['other_feature'])`\n",
        "\n",
        "4. **Chi-Square Test**\n",
        "\n",
        "   * For categorical data, test whether missingness is independent of other features.\n",
        "\n",
        "5. **Pattern Detection Tools**\n",
        "\n",
        "   * Python libraries like `missingno` help identify missing data patterns:\n",
        "\n",
        "   ```python\n",
        "   import missingno as msno\n",
        "   msno.matrix(df)\n",
        "   msno.bar(df)\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "* **MCAR:** Safe to use deletion methods\n",
        "* **MAR:** Use imputation techniques based on other features\n",
        "* **MNAR:** Requires careful modeling or domain knowledge\n",
        "\n",
        "Understanding the **pattern of missing data** ensures more **accurate handling** and reduces bias in analysis.\n"
      ],
      "metadata": {
        "id": "BilWbgfhiXPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9:** Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
        "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
        "can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "Here‚Äôs a **clear, exam-ready answer** for **Q9** üëá\n",
        "\n",
        "---\n",
        "\n",
        "## **Evaluating Machine Learning Models on Imbalanced Data**\n",
        "\n",
        "In medical diagnosis, if the dataset is **highly imbalanced** (e.g., few patients have a disease), standard accuracy is **not reliable**, because predicting the majority class will give high accuracy but fail to detect the minority (disease) cases.\n",
        "\n",
        "---\n",
        "\n",
        "### **Strategies to Evaluate Performance**\n",
        "\n",
        "1. **Use Appropriate Metrics**\n",
        "\n",
        "   * **Precision:** Fraction of correctly predicted positive cases among all predicted positives\n",
        "   * **Recall (Sensitivity):** Fraction of correctly predicted positive cases among actual positives\n",
        "   * **F1-Score:** Harmonic mean of precision and recall; balances both\n",
        "   * **ROC-AUC (Receiver Operating Characteristic ‚Äì Area Under Curve):** Measures the tradeoff between true positive rate and false positive rate\n",
        "   * **PR-AUC (Precision-Recall AUC):** Especially useful for highly imbalanced datasets\n",
        "\n",
        "2. **Confusion Matrix Analysis**\n",
        "\n",
        "   * Examine **true positives, false positives, true negatives, false negatives**\n",
        "   * Helps understand how many minority class cases are correctly predicted\n",
        "\n",
        "3. **Resampling Techniques**\n",
        "\n",
        "   * **Up-sampling / Down-sampling:** Balance the classes before training\n",
        "   * **SMOTE (Synthetic Minority Oversampling Technique):** Generate synthetic minority class examples\n",
        "\n",
        "4. **Stratified Sampling**\n",
        "\n",
        "   * Ensure **train-test splits maintain the original class ratio**\n",
        "   * Prevents model bias during training or testing\n",
        "\n",
        "5. **Cost-Sensitive Learning**\n",
        "\n",
        "   * Assign **higher misclassification penalties** to the minority class\n",
        "   * Encourages the model to pay more attention to critical cases\n",
        "\n",
        "6. **Cross-Validation with Stratification**\n",
        "\n",
        "   * Ensures each fold has a **proportional representation** of the minority class\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaway**\n",
        "\n",
        "* Do **not rely solely on accuracy**\n",
        "* Focus on metrics that **emphasize the minority class** (recall, F1-score, ROC-AUC)\n",
        "* Combine **evaluation metrics** with **resampling or cost-sensitive methods** to get a reliable assessment.\n"
      ],
      "metadata": {
        "id": "ID5QATw7ioSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10:** When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
        "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
        "balance the dataset and down-sample the majority class?\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "Here‚Äôs a **clear, exam-ready answer** for **Q10** üëá\n",
        "\n",
        "---\n",
        "\n",
        "## **Balancing an Unbalanced Customer Satisfaction Dataset**\n",
        "\n",
        "When most customers report being satisfied, the dataset is **imbalanced**, with the majority class (satisfied) dominating the minority class (dissatisfied). This can bias machine learning models toward predicting satisfaction for everyone.\n",
        "\n",
        "To balance the dataset, you can **down-sample the majority class** or use other resampling techniques.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Down-Sampling the Majority Class**\n",
        "\n",
        "**Definition:** Randomly remove samples from the majority class to match the size of the minority class.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Up-Sampling the Minority Class**\n",
        "\n",
        "**Definition:** Duplicate or generate synthetic samples of the minority class to match the majority class.\n",
        "\n",
        "* Can use **SMOTE** for generating synthetic minority samples.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Combination Methods**\n",
        "\n",
        "* **SMOTE + Down-Sampling:** Balance by reducing majority and enhancing minority classes simultaneously.\n",
        "* **Weighted Loss Functions:** Assign higher misclassification penalties to the minority class during training.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Considerations When Down-Sampling**\n",
        "\n",
        "* May **discard useful information** from majority class\n",
        "* Works well when **majority class is very large**\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "Down-sampling is an effective way to **balance imbalanced datasets** when the majority class dominates. Always combine it with careful **model evaluation using metrics like recall, F1-score, or ROC-AUC** to ensure the minority class is well represented.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7pRuSunOi28k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Example dataset\n",
        "data = pd.DataFrame({\n",
        "    'Satisfaction': ['Satisfied']*90 + ['Dissatisfied']*10,\n",
        "    'Feature': range(100)\n",
        "})\n",
        "\n",
        "# Separate majority and minority classes\n",
        "majority = data[data.Satisfaction=='Satisfied']\n",
        "minority = data[data.Satisfaction=='Dissatisfied']\n",
        "\n",
        "# Down-sample majority class\n",
        "majority_downsampled = resample(\n",
        "    majority,\n",
        "    replace=False,     # No replacement\n",
        "    n_samples=len(minority),  # Match minority class size\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Combine minority class with downsampled majority class\n",
        "balanced_data = pd.concat([majority_downsampled, minority])\n",
        "print(balanced_data['Satisfaction'].value_counts())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkX36a5jjKW-",
        "outputId": "edeff1dd-1743-4134-e54b-e23d5d7ae45f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Satisfaction\n",
            "Satisfied       10\n",
            "Dissatisfied    10\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q11:** You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
        "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
        "balance the dataset and up-sample the minority class?\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "Balancing an Unbalanced Dataset with Rare Events\n",
        "\n",
        "When the dataset has a low percentage of rare events (minority class), models may fail to predict these events accurately. To handle this, we can up-sample the minority class to balance the dataset.\n",
        "\n",
        "1. Random Over-Sampling\n",
        "\n",
        "Definition: Duplicate existing minority class samples to increase their frequency.\n",
        "\n",
        "Python Example:\n"
      ],
      "metadata": {
        "id": "2a2A_5c2jPQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Example dataset\n",
        "data = pd.DataFrame({\n",
        "    'Event': ['No']*95 + ['Yes']*5,\n",
        "    'Feature': range(100)\n",
        "})\n",
        "\n",
        "# Separate majority and minority classes\n",
        "majority = data[data.Event=='No']\n",
        "minority = data[data.Event=='Yes']\n",
        "\n",
        "# Up-sample minority class\n",
        "minority_upsampled = resample(\n",
        "    minority,\n",
        "    replace=True,           # With replacement\n",
        "    n_samples=len(majority),  # Match majority size\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Combine with majority class\n",
        "balanced_data = pd.concat([majority, minority_upsampled])\n",
        "print(balanced_data['Event'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-zgovGbh2P3",
        "outputId": "7866bec4-1f33-4e7c-e9f8-db2ab26b3421"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Event\n",
            "No     95\n",
            "Yes    95\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. SMOTE (Synthetic Minority Over-sampling Technique)\n",
        "\n",
        "Generates synthetic samples by interpolating between existing minority samples instead of simple duplication.\n",
        "\n",
        "Reduces overfitting compared to random duplication.\n",
        "\n",
        "Python Example with imblearn:"
      ],
      "metadata": {
        "id": "CwASbrNVjf8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Example dataset\n",
        "data = pd.DataFrame({\n",
        "    'Event': ['No']*95 + ['Yes']*5,  # Minority class = 5 samples\n",
        "    'Feature': range(100)\n",
        "})\n",
        "\n",
        "# Separate features and target\n",
        "X = data[['Feature']]\n",
        "y = data['Event']\n",
        "\n",
        "# Determine the number of minority samples\n",
        "minority_count = y.value_counts()['Yes']\n",
        "\n",
        "# Set k_neighbors to a safe value\n",
        "k_neighbors = min(5, minority_count - 1)  # Must be < minority samples\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
        "X_res, y_res = smote.fit_resample(X, y)\n",
        "\n",
        "# Combine back into a DataFrame for convenience\n",
        "balanced_data = pd.DataFrame(X_res, columns=['Feature'])\n",
        "balanced_data['Event'] = y_res\n",
        "\n",
        "# Check class distribution\n",
        "print(balanced_data['Event'].value_counts())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lGn8eMJjgm0",
        "outputId": "7edff083-54b0-4836-f968-82be8d6fb987"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Event\n",
            "No     95\n",
            "Yes    95\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Other Up-sampling Techniques\n",
        "\n",
        "ADASYN (Adaptive Synthetic Sampling): Focuses more on minority samples that are harder to classify.\n",
        "\n",
        "Combination with Down-Sampling: Reduce majority class slightly and up-sample minority class to balance dataset efficiently.\n",
        "\n",
        "4. Key Considerations\n",
        "\n",
        "Avoid overfitting by not creating too many duplicates.\n",
        "\n",
        "Always evaluate model using metrics for imbalanced data (Recall, F1-score, ROC-AUC).\n",
        "\n",
        "Up-sampling is particularly useful when minority class is small but important (fraud, rare disease, equipment failure).\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Up-sampling the minority class ensures that the model learns to detect rare events, improving prediction performance on critical but infrequent outcomes."
      ],
      "metadata": {
        "id": "zvyhi1uWjoDF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n-t24DQOjgqS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}