{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsIMNf2+3h2gZcHy0Gc9xo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanchandhole/Data-Scientist/blob/main/18th_march_feature_eng_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.** What is the Filter method in feature selection, and how does it work?\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "---\n",
        "\n",
        "## **Filter Method in Feature Selection**\n",
        "\n",
        "### **Definition**\n",
        "\n",
        "The **Filter method** is a **feature selection technique** that evaluates the importance of features **independently of any machine learning model**.\n",
        "It selects or removes features based on **statistical measures** or intrinsic properties of the data.\n",
        "\n",
        "---\n",
        "\n",
        "### **How It Works**\n",
        "\n",
        "1. Calculate a **relevance score** for each feature using statistical tests or metrics.\n",
        "2. Rank features based on the score.\n",
        "3. Select the top features or remove irrelevant ones before model training.\n",
        "\n",
        "---\n",
        "\n",
        "### **Common Metrics Used**\n",
        "\n",
        "* **Correlation coefficient** (Pearson, Spearman) â€“ for numerical features\n",
        "* **Chi-square test** â€“ for categorical features\n",
        "* **ANOVA F-test** â€“ for comparing groups\n",
        "* **Mutual information** â€“ measures dependency between feature and target\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "\n",
        "Suppose you have a dataset to predict customer churn:\n",
        "\n",
        "* Feature: `Age`, `Salary`, `Account Type`, `Region`\n",
        "* Target: `Churn`\n",
        "\n",
        "**Step 1:** Calculate correlation with the target\n",
        "**Step 2:** Select features with high correlation (`Age`, `Salary`) and remove low-correlation features (`Region`)\n",
        "\n",
        "---\n",
        "\n",
        "### **Advantages**\n",
        "\n",
        "* Fast and simple\n",
        "* Works well with **high-dimensional datasets**\n",
        "* Reduces overfitting\n",
        "\n",
        "### **Disadvantages**\n",
        "\n",
        "* Ignores **feature interactions**\n",
        "* May select redundant features\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "The **Filter method** is a **pre-processing step** that quickly identifies relevant features using statistical measures, independent of any machine learning algorithm.\n"
      ],
      "metadata": {
        "id": "UA0VnIoBkrw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2.** How does the Wrapper method differ from the Filter method in feature selection?\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "---\n",
        "\n",
        "## **Wrapper Method vs Filter Method in Feature Selection**\n",
        "\n",
        "Feature selection techniques can be broadly categorized into **Filter**, **Wrapper**, and **Embedded** methods. Here we compare **Wrapper** and **Filter** methods.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Filter Method**\n",
        "\n",
        "* **Model-agnostic:** Selects features **independently of any machine learning model**.\n",
        "* **Based on statistical measures**: correlation, chi-square, ANOVA, mutual information, etc.\n",
        "* **Advantages:** Fast, simple, works well for high-dimensional data.\n",
        "* **Disadvantages:** Ignores interactions between features; may include redundant features.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Wrapper Method**\n",
        "\n",
        "* **Model-based:** Uses a **machine learning model** to evaluate subsets of features.\n",
        "* **How it works:**\n",
        "\n",
        "  1. Select a subset of features\n",
        "  2. Train the model on this subset\n",
        "  3. Evaluate model performance (e.g., accuracy, F1-score)\n",
        "  4. Repeat for different subsets to find the best-performing set\n",
        "* **Search strategies:** Forward selection, backward elimination, recursive feature elimination (RFE)\n",
        "* **Advantages:** Accounts for **feature interactions** and model performance\n",
        "* **Disadvantages:** Computationally expensive, especially for large datasets\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison Table**\n",
        "\n",
        "| Aspect               | Filter Method           | Wrapper Method                      |\n",
        "| -------------------- | ----------------------- | ----------------------------------- |\n",
        "| Model Dependency     | Independent             | Model-based                         |\n",
        "| Computation          | Fast                    | Slow (computationally expensive)    |\n",
        "| Feature Interactions | Ignored                 | Considered                          |\n",
        "| Accuracy             | May be lower            | Usually higher (better performance) |\n",
        "| Example              | Correlation, Chi-square | Recursive Feature Elimination (RFE) |\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "\n",
        "* **Filter:** Select features with high correlation with target.\n",
        "* **Wrapper:** Use RFE with a decision tree to iteratively select features that maximize model accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "* **Filter methods** are faster and simpler, suitable for preprocessing.\n",
        "* **Wrapper methods** are more accurate and account for interactions but are computationally expensive.\n"
      ],
      "metadata": {
        "id": "wDiFHnJyk6vk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3.** What are some common techniques used in Embedded feature selection methods?\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "---\n",
        "\n",
        "## **Embedded Feature Selection Methods**\n",
        "\n",
        "### **Definition**\n",
        "\n",
        "Embedded methods perform **feature selection during the model training process**.\n",
        "Unlike **Filter methods** (independent of model) or **Wrapper methods** (iterative search), embedded methods **integrate feature selection into the learning algorithm itself**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Common Techniques**\n",
        "\n",
        "1. **Lasso Regression (L1 Regularization)**\n",
        "\n",
        "   * Adds a penalty proportional to the **absolute value of coefficients**.\n",
        "   * Encourages **sparse coefficients**, automatically shrinking some feature weights to zero.\n",
        "   * Features with zero coefficients are **effectively removed**.\n",
        "\n",
        "2. **Ridge Regression (L2 Regularization)**\n",
        "\n",
        "   * Adds a penalty proportional to the **square of coefficients**.\n",
        "   * Doesnâ€™t shrink features to zero, but reduces the effect of less important features.\n",
        "\n",
        "3. **Elastic Net**\n",
        "\n",
        "   * Combines **L1 and L2 penalties**.\n",
        "   * Helps in selecting features when features are correlated.\n",
        "\n",
        "4. **Tree-based Models**\n",
        "\n",
        "   * Algorithms like **Decision Trees, Random Forests, Gradient Boosting** naturally perform feature selection.\n",
        "   * Use **feature importance scores** to identify key features.\n",
        "   * Example: `model.feature_importances_` in scikit-learn.\n",
        "\n",
        "5. **Regularized Logistic Regression**\n",
        "\n",
        "   * Logistic regression with **L1 or L2 regularization** selects important features for classification tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### **Advantages**\n",
        "\n",
        "* Performs feature selection **during model training**\n",
        "* Accounts for **feature interactions**\n",
        "* Less computationally expensive than wrapper methods\n",
        "\n",
        "### **Disadvantages**\n",
        "\n",
        "* Model-dependent\n",
        "* Selected features may vary with **different algorithms or hyperparameters**\n",
        "\n",
        "---\n",
        "\n",
        "### **Example Workflow**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "Embedded methods combine **feature selection and model training**, making them **efficient and accurate**, especially for high-dimensional datasets.\n"
      ],
      "metadata": {
        "id": "HQCE9JKWlHVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import pandas as pd\n",
        "\n",
        "# Load California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "y = housing.target\n",
        "\n",
        "# Lasso regression for embedded feature selection\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X, y)\n",
        "\n",
        "# Select features with non-zero coefficients\n",
        "selected_features = X.columns[lasso.coef_ != 0]\n",
        "print(\"Selected features:\", selected_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6CoDS1BlXwX",
        "outputId": "1a6c4405-77aa-4768-b19e-8ed1be5fa3d4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features: Index(['MedInc', 'HouseAge', 'Population', 'AveOccup', 'Latitude',\n",
            "       'Longitude'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5.** In which situations would you prefer using the Filter method over the Wrapper method for feature\n",
        "selection?\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "## **When to Prefer Filter Method Over Wrapper Method**\n",
        "\n",
        "The **Filter method** and **Wrapper method** are both used for feature selection, but they have different strengths. You would prefer **Filter methods** in the following situations:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. High-Dimensional Datasets**\n",
        "\n",
        "* When the dataset has **thousands or millions of features** (e.g., text data, gene expression data)\n",
        "* Filter methods are **computationally efficient** and fast\n",
        "* Wrapper methods would be **too slow**, as they evaluate subsets of features with a model\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Need for Model-Agnostic Selection**\n",
        "\n",
        "* Filter methods are **independent of the machine learning algorithm**\n",
        "* Useful if you want to **preprocess data** before trying multiple models\n",
        "* Wrapper methods are **model-dependent** (feature selection changes with different algorithms)\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Quick Feature Screening**\n",
        "\n",
        "* When you need a **preliminary selection** to remove irrelevant features quickly\n",
        "* Helps reduce noise and improve model training speed\n",
        "* Example: Selecting features based on correlation with the target\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Avoiding Overfitting in Small Datasets**\n",
        "\n",
        "* Filter methods do not use the model, so they **do not overfit** to the training set\n",
        "* Wrapper methods can overfit, especially when the dataset is **small**\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Simple and Interpretable Criteria**\n",
        "\n",
        "* Filter methods use **statistical measures** (correlation, chi-square, ANOVA, mutual information)\n",
        "* Easy to **understand and justify** feature selection\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "\n",
        "* **Filter Method:** Removing features with low correlation with the target before model training\n",
        "* **Wrapper Method:** Using Recursive Feature Elimination (RFE) with a classifier to find the best subset\n",
        "\n",
        "**Scenario:**\n",
        "\n",
        "* Text classification with 10,000 words â†’ Filter method to reduce features based on chi-square scores\n",
        "* Gene expression dataset with 20,000 genes â†’ Filter method for quick screening\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "Use the **Filter method** when you need **speed, scalability, model-agnostic selection, or preliminary feature screening**, especially for **high-dimensional datasets**."
      ],
      "metadata": {
        "id": "43PN3R4uld5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6.** In a telecom company, you are working on a project to develop a predictive model for customer churn.\n",
        "You are unsure of which features to include in the model because the dataset contains several different\n",
        "ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method.\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "Hereâ€™s a **clear, step-by-step answer** for **Q6** ðŸ‘‡\n",
        "\n",
        "---\n",
        "\n",
        "## **Using the Filter Method for Feature Selection in a Telecom Churn Project**\n",
        "\n",
        "When predicting **customer churn**, you often have a dataset with many features such as:\n",
        "\n",
        "* Customer demographics (age, gender, location)\n",
        "* Usage metrics (call duration, data usage, number of complaints)\n",
        "* Billing info (monthly charges, payment method)\n",
        "\n",
        "To select the **most relevant features**, the **Filter method** can be applied as follows:\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 1: Understand the Data**\n",
        "\n",
        "* Identify feature types:\n",
        "\n",
        "  * **Numerical:** Age, monthly charges, call duration\n",
        "  * **Categorical:** Gender, contract type, payment method\n",
        "* Identify the **target variable**: `Churn` (Yes/No)\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Choose Statistical Measures**\n",
        "\n",
        "* **Numerical Features vs Target:**\n",
        "\n",
        "  * Use **Correlation Coefficient** (Pearson or Spearman) to measure linear relationship with churn\n",
        "  * Features highly correlated with churn are likely relevant\n",
        "* **Categorical Features vs Target:**\n",
        "\n",
        "  * Use **Chi-square test** or **ANOVA F-test**\n",
        "  * Example: Test if contract type or payment method is associated with churn\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Calculate Scores**\n",
        "\n",
        "* Compute correlation scores for numeric features\n",
        "* Compute chi-square or ANOVA scores for categorical features\n",
        "\n",
        "```python\n",
        "\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 4: Select Top Features**\n",
        "\n",
        "* Rank features by score\n",
        "* Keep features with **high scores** and remove low-scoring features\n",
        "* Example:\n",
        "\n",
        "  * Keep: `MonthlyCharges`, `Contract_TwoYear`, `Tenure`\n",
        "  * Remove: `Gender`, `City` (if low relevance)\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 5: Validate Selection**\n",
        "\n",
        "* Check if selected features improve model performance\n",
        "* Filter methods are **fast**, but you can later combine with **Wrapper or Embedded methods** for fine-tuning\n",
        "\n",
        "---\n",
        "\n",
        "### **Advantages of Filter Method in This Case**\n",
        "\n",
        "1. Works **independently of the model**, fast for many features\n",
        "2. Reduces noise and irrelevant data\n",
        "3. Prevents overfitting in small datasets\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "Using the **Filter method**, you can efficiently identify **pertinent attributes** for the churn model by applying **correlation, chi-square, or ANOVA** tests, ranking features, and selecting the most relevant ones for training.\n"
      ],
      "metadata": {
        "id": "2KGQWHAdmGN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
        "\n",
        "# --------------------------\n",
        "# Step 1: Create sample dataset\n",
        "# --------------------------\n",
        "data = pd.DataFrame({\n",
        "    'Contract': ['Month-to-month', 'Two year', 'One year', 'Month-to-month', 'Two year', 'One year'],\n",
        "    'PaymentMethod': ['Electronic check', 'Mailed check', 'Bank transfer', 'Credit card', 'Electronic check', 'Bank transfer'],\n",
        "    'MonthlyCharges': [70, 80, 60, 90, 85, 75],\n",
        "    'Tenure': [1, 24, 12, 3, 36, 18],\n",
        "    'Churn': ['Yes', 'No', 'No', 'Yes', 'No', 'No']\n",
        "})\n",
        "\n",
        "# --------------------------\n",
        "# Step 2: Prepare features and target\n",
        "# --------------------------\n",
        "# Encode categorical variables\n",
        "X_cat = pd.get_dummies(data[['Contract', 'PaymentMethod']], drop_first=True)\n",
        "y = data['Churn'].map({'No': 0, 'Yes': 1})  # Encode target\n",
        "\n",
        "# --------------------------\n",
        "# Step 3: Apply Chi-square test\n",
        "# --------------------------\n",
        "chi_selector = SelectKBest(chi2, k='all')\n",
        "chi_selector.fit(X_cat, y)\n",
        "\n",
        "# --------------------------\n",
        "# Step 4: View scores\n",
        "# --------------------------\n",
        "scores = pd.DataFrame({'Feature': X_cat.columns, 'Score': chi_selector.scores_})\n",
        "scores = scores.sort_values(by='Score', ascending=False)\n",
        "print(scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToT7pRb3mXog",
        "outputId": "c9364c8b-f74b-4d62-cace-ce4a0dac1f6e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Feature  Score\n",
            "2       PaymentMethod_Credit card   2.00\n",
            "0               Contract_One year   1.00\n",
            "1               Contract_Two year   1.00\n",
            "4      PaymentMethod_Mailed check   0.50\n",
            "3  PaymentMethod_Electronic check   0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7.** You are working on a project to predict the outcome of a soccer match. You have a large dataset with\n",
        "many features, including player statistics and team rankings. Explain how you would use the Embedded\n",
        "method to select the most relevant features for the model.\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "---\n",
        "\n",
        "## **Using Embedded Feature Selection for Soccer Match Outcome Prediction**\n",
        "\n",
        "Suppose you are predicting the outcome of a soccer match (Win/Loss/Draw) using a dataset that contains:\n",
        "\n",
        "* Player statistics (goals scored, assists, passes completed)\n",
        "* Team rankings (league position, recent form)\n",
        "* Other match-related features (home/away, weather, referee)\n",
        "\n",
        "To select the **most relevant features**, you can use **Embedded methods**, which perform feature selection **during model training**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 1: Choose a Model with Built-in Feature Selection**\n",
        "\n",
        "Embedded methods rely on models that **penalize or rank features** automatically:\n",
        "\n",
        "* **Tree-based models:** Decision Trees, Random Forests, Gradient Boosting\n",
        "* **Regularized models:** Logistic Regression with L1 (Lasso) or Elastic Net\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Train the Model**\n",
        "\n",
        "* Fit the chosen model to your data:\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# X = feature matrix, y = target (Win/Loss/Draw)\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X, y)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Evaluate Feature Importance**\n",
        "\n",
        "* Most embedded methods provide **feature importance scores**:\n",
        "\n",
        "  * **Random Forest / Gradient Boosting:** `model.feature_importances_`\n",
        "  * **Lasso / Elastic Net:** Coefficients close to zero â†’ unimportant\n",
        "* Rank features based on importance.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf_model.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(feature_importances)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 4: Select Top Features**\n",
        "\n",
        "* Keep the most important features for model training (e.g., top 10â€“20)\n",
        "* Remove low-importance features that contribute little or add noise\n",
        "\n",
        "---\n",
        "\n",
        "### **Advantages of Embedded Method**\n",
        "\n",
        "1. **Accounts for feature interactions** during model training\n",
        "2. **Efficient** because feature selection happens alongside model learning\n",
        "3. Reduces overfitting by removing irrelevant or redundant features\n",
        "4. Often provides **better predictive performance** than simple filter methods\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 5: Optional â€“ Combine with Hyperparameter Tuning**\n",
        "\n",
        "* You can **tune the model** (number of trees, L1 penalty, etc.) and **reevaluate feature importance**\n",
        "* Ensures that selected features are robust and contribute meaningfully to predictions\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "Using **Embedded feature selection**, you can efficiently identify **the most relevant player stats, team metrics, and match features** while training a predictive model. This improves **accuracy, reduces complexity**, and makes the model more interpretable.\n",
        "\n"
      ],
      "metadata": {
        "id": "YbmlWdsom3Nb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8.** You are working on a project to predict the price of a house based on its features, such as size, location,\n",
        "and age. You have a limited number of features, and you want to ensure that you select the most important\n",
        "ones for the model. Explain how you would use the Wrapper method to select the best set of features for the\n",
        "predictor.\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **Using the Wrapper Method for House Price Prediction**\n",
        "\n",
        "Suppose you are predicting house prices using features such as:\n",
        "\n",
        "* **Size** (square feet, number of bedrooms)\n",
        "* **Location** (city, neighborhood)\n",
        "* **Age of the house**\n",
        "* **Other amenities** (garage, garden, pool)\n",
        "\n",
        "You want to **select the best combination of features** to maximize predictive performance. The **Wrapper method** is ideal because it **evaluates feature subsets using the actual model**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 1: Choose a Predictive Model**\n",
        "\n",
        "* Select a model appropriate for regression, e.g.:\n",
        "\n",
        "  * Linear Regression\n",
        "  * Random Forest Regressor\n",
        "  * Gradient Boosting Regressor\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Define a Search Strategy**\n",
        "\n",
        "Wrapper methods explore **different subsets of features** and evaluate the model performance for each subset. Common strategies:\n",
        "\n",
        "1. **Forward Selection**\n",
        "\n",
        "   * Start with no features\n",
        "   * Add one feature at a time that improves model performance\n",
        "   * Repeat until no further improvement\n",
        "2. **Backward Elimination**\n",
        "\n",
        "   * Start with all features\n",
        "   * Remove the least important feature iteratively\n",
        "   * Stop when performance decreases\n",
        "3. **Recursive Feature Elimination (RFE)**\n",
        "\n",
        "   * Fit the model, rank features by importance\n",
        "   * Remove the least important feature(s)\n",
        "   * Repeat until desired number of features is left\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Evaluate Feature Subsets**\n",
        "\n",
        "* For each subset, train the model and evaluate performance metrics, e.g.:\n",
        "\n",
        "  * **RÂ² score**\n",
        "  * **Mean Squared Error (MSE)**\n",
        "\n",
        "* Select the subset that **maximizes accuracy** and reduces complexity.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### **Advantages of Wrapper Method**\n",
        "\n",
        "1. **Considers feature interactions**\n",
        "2. **Optimizes for model performance**\n",
        "3. **More accurate feature selection** than simple filter methods\n",
        "\n",
        "---\n",
        "\n",
        "### **Disadvantages**\n",
        "\n",
        "* Computationally expensive if **many features** are present\n",
        "* May overfit on small datasets\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "The **Wrapper method** is suitable when the number of features is limited and the goal is to find the **best-performing combination of features** for predicting house prices.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ains7xr-nXXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.feature_selection import RFE\n",
        "import pandas as pd\n",
        "\n",
        "# Sample dataset\n",
        "X = pd.DataFrame({\n",
        "    'Size': [1200, 1500, 800, 900, 1300],\n",
        "    'Bedrooms': [3, 4, 2, 2, 3],\n",
        "    'Age': [10, 5, 20, 15, 8],\n",
        "    'Garage': [1, 1, 0, 0, 1]\n",
        "})\n",
        "y = [300000, 400000, 200000, 220000, 320000]\n",
        "\n",
        "# Initialize model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Recursive Feature Elimination\n",
        "rfe = RFE(model, n_features_to_select=2)\n",
        "rfe.fit(X, y)\n",
        "\n",
        "# Selected features\n",
        "selected_features = X.columns[rfe.support_]\n",
        "print(\"Selected features:\", selected_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4UJVYIEnuoA",
        "outputId": "d647d211-0f99-4b2b-f6c9-39519f7a753d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features: Index(['Bedrooms', 'Garage'], dtype='object')\n"
          ]
        }
      ]
    }
  ]
}