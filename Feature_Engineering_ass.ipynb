{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMT6sbhG8rDjKCYauHGSCwy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanchandhole/Data-Scientist/blob/main/Feature_Engineering_ass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.**What is a parameter?\n",
        "\n",
        "**Ans:**In machine learning, a parameter is an internal variable within a model that the model learns and adjusts during the training process to improve its ability to make accurate predictions"
      ],
      "metadata": {
        "id": "ib-iLwL4IDTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.**What is correlation?\n",
        "What does negative correlation mean?\n",
        "\n",
        "**Ans:**Correlation is a statistical measure that indicates the extent to which two variables are related or vary together.\n",
        "\n",
        "It can be positive, negative, or zero (no correlation).\n",
        "\n",
        "Correlation does not imply causation; it only indicates a relationship.\n",
        "\n",
        "\n",
        "What is Negative Correlation?\n",
        "\n",
        "Negative correlation, also known as inverse correlation, means that as one variable increases, the other variable tends to decrease, and vice versa.\n",
        "A perfect negative correlation is represented by a correlation coefficient of -1."
      ],
      "metadata": {
        "id": "pT5NuruXI4Nw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.**Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "**Ans:** Machine learning is type of artificial intelligence that allow computer to learn and improve without explicit program.\n",
        "\n",
        "their are 4 types of component in Machine learning.\n",
        "Data , Model, Algorithm, Predications.\n",
        "\n",
        "Data:\n",
        "Machine learning algorithms require data to learn from. This data can be structured (like a database) or unstructured (like images or text).\n",
        "\n",
        "Algorithms:\n",
        "These are the mathematical procedures that enable computers to analyze data, identify patterns, and make predictions.\n",
        "\n",
        "Models:\n",
        "These are the representations of the patterns and relationships learned from the data by the algorithms.\n",
        "\n",
        "Predictions:\n",
        "Once the model is trained, it can be used to make predictions or decisions on new, unseen data."
      ],
      "metadata": {
        "id": "pHo_Rjf3JhAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.**How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "**Ans:-** Lower Loss = Better Performance: A lower loss value indicates that the model's predictions are closer to the actual values, suggesting better performance.\n",
        "\n",
        "High Loss = Poor Performance: Conversely, a high loss value means the model's predictions are significantly off, indicating poor performance"
      ],
      "metadata": {
        "id": "7U-bH8NOOLM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.**What are continuous and categorical variables?\n",
        "\n",
        "**Ans:** 1. Continuous Variables:\n",
        "Definition:\n",
        "Continuous variables are numerical values that can take on any value within a given range, including decimals and fractions.\n",
        "\n",
        "Examples:\n",
        "Height (e.g., 1.75 meters)\n",
        "Weight (e.g., 70.5 kg)\n",
        "Temperature (e.g., 25.3 degrees Celsius)\n",
        "Time (e.g., 10:34:22)\n",
        "\n",
        "Characteristics:\n",
        "They are typically measured rather than counted.\n",
        "There are infinitely many possible values between any two values.\n",
        "\n",
        "2. Categorical Variables:\n",
        "Definition:\n",
        "Categorical variables represent data that can be classified into distinct groups or categories.\n",
        "\n",
        "Examples:\n",
        "Gender (male, female)\n",
        "Eye color (blue, brown, green)\n",
        "Car make (Toyota, Honda, Ford)\n",
        "City (Nagpur, Mumbai, Delhi)\n",
        "\n",
        "Characteristics:\n",
        "They are typically named or labeled rather than measured.\n",
        "There are a finite number of possible values.\n",
        "\n",
        "Types of Categorical Variables:\n",
        "Nominal: Categories have no inherent order or ranking (e.g., colors, types of fruits).\n",
        "Ordinal: Categories have a meaningful order or ranking (e.g., education level, movie ratings)."
      ],
      "metadata": {
        "id": "eACWXAtcTbmi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Continuous Variables:\n",
        "Definition:\n",
        "Continuous variables are numerical values that can take on any value within a given range, including decimals and fractions.\n",
        "Examples:\n",
        "Height (e.g., 1.75 meters)\n",
        "Weight (e.g., 70.5 kg)\n",
        "Temperature (e.g., 25.3 degrees Celsius)\n",
        "Time (e.g., 10:34:22)\n",
        "Characteristics:\n",
        "They are typically measured rather than counted.\n",
        "There are infinitely many possible values between any two values.\n",
        "2. Categorical Variables:\n",
        "Definition:\n",
        "Categorical variables represent data that can be classified into distinct groups or categories.\n",
        "Examples:\n",
        "Gender (male, female)\n",
        "Eye color (blue, brown, green)\n",
        "Car make (Toyota, Honda, Ford)\n",
        "City (Nagpur, Mumbai, Delhi)\n",
        "Characteristics:\n",
        "They are typically named or labeled rather than measured.\n",
        "There are a finite number of possible values.\n",
        "Types of Categorical Variables:\n",
        "Nominal: Categories have no inherent order or ranking (e.g., colors, types of fruits).\n",
        "Ordinal: Categories have a meaningful order or ranking (e.g., education level, movie ratings)."
      ],
      "metadata": {
        "id": "NhaRr0yfUygM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.**How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "**Ans:**To handle categorical variables in machine learning, you need to convert them into a numerical format that algorithms can understand. Common techniques include one-hot encoding, label encoding, ordinal encoding, and target encoding, each with its strengths and weaknesses.\n",
        "Here's a breakdown of these techniques:\n",
        "\n",
        "1.One-Hot Encoding:\n",
        "Concept: Creates a binary column (0 or 1) for each unique category in a categorical variable.\n",
        "Example: If a variable \"Color\" has categories \"Red\", \"Blue\", and \"Green\", one-hot encoding would create three new columns: \"Color_Red\", \"Color_Blue\", and \"Color_Green\".\n",
        "Pros: Simple to understand and implement; preserves the original information.\n",
        "Cons: Can lead to high dimensionality, especially with many categories.\n",
        "\n",
        "2.Label Encoding:\n",
        "Concept: Assigns a unique numerical value to each category, starting from 0 or 1.\n",
        "Example: If a variable \"Size\" has categories \"Small\", \"Medium\", and \"Large\", label encoding could map them to 0, 1, and 2 respectively.\n",
        "Pros: Simple and computationally efficient.\n",
        "Cons: Can create an ordinal relationship between categories even if they are not inherently ordered, which can mislead the model.\n",
        "\n",
        "3.Ordinal Encoding:\n",
        "Concept:\n",
        "Assigns numerical values to each category based on their natural order or hierarchy.\n",
        "Example:\n",
        "If a variable \"Satisfaction\" has categories \"Very Dissatisfied\", \"Dissatisfied\", \"Neutral\", \"Satisfied\", and \"Very Satisfied\", ordinal encoding could map them to 0, 1, 2, 3, and 4 respectively.\n",
        "Pros:\n",
        "Preserves the ordinal relationship between categories, which is useful for models that can leverage this information.\n",
        "Cons:\n",
        "Only suitable for variables with a clear and meaningful order.\n",
        "\n",
        "4.Target Encoding:\n",
        "Concept:\n",
        "Replaces each category with the mean of the target variable for that category.\n",
        "Example:\n",
        "If you're predicting customer churn, and a category \"Customer_Type\" has categories \"A\" and \"B\", target encoding could replace \"A\" with the average churn rate for customers of type \"A\" and \"B\" with the average churn rate for customers of type \"B\".\n",
        "Pros:\n",
        "Can capture complex relationships between categorical variables and the target variable.\n",
        "Cons:\n",
        "Requires careful handling to avoid overfitting and leakage of target information.\n",
        "\n",
        "5.Binary Encoding:\n",
        "Concept: Converts each category into its binary representation and then splits the binary digits into separate columns.\n",
        "Example: If a variable \"Color\" has categories \"Red\", \"Blue\", and \"Green\", binary encoding could map them to 00, 01, and 10 respectively, then create two columns: \"Color_0\" and \"Color_1\".\n",
        "Pros: More compact than one-hot encoding, reducing dimensionality.\n",
        "Cons: Can be more complex to implement and interpret.\n",
        "\n",
        "6.Frequency Encoding:\n",
        "Concept: Replaces each category with the count of how often it appears in the dataset.\n",
        "Example: If a variable \"City\" has categories \"New York\" (100 times), \"London\" (50 times), and \"Paris\" (25 times), frequency encoding could map them to 100, 50, and 25 respectively.\n",
        "Pros: Simple and effective for capturing the importance of categories.\n",
        "Cons: Can be problematic if some categories are very rare.\n",
        "\n",
        "7.Other Techniques:\n",
        "CatBoost Encoding:\n",
        "A technique used by the CatBoost algorithm to handle categorical features during the training phase.\n",
        "Hash Encoding:\n",
        "A technique that uses hashing algorithms to map categorical values to numerical ones."
      ],
      "metadata": {
        "id": "yK877mGoUy2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.**What do you mean by training and testing a dataset?\n",
        "\n",
        "**Ans:** **Training Dataset:**\n",
        "\n",
        "This is the data used to \"train\" or \"teach\" the machine learning model.\n",
        "The model learns from the patterns and relationships within this data.\n",
        "The model is exposed to the training data to identify features and make predictions.\n",
        "The goal is to have the model learn the underlying structure of the data.\n",
        "\n",
        "**Testing Dataset:**\n",
        "\n",
        "This dataset is used to evaluate the performance of the trained model.\n",
        "It's crucial that the testing data is unseen by the model during the training process.\n",
        "This ensures that the model's performance is a true reflection of how well it generalizes to new, unseen data.\n",
        "By comparing the model's predictions on the test data with the actual values, you can assess its accuracy and identify areas for improvement."
      ],
      "metadata": {
        "id": "q25Z581pV3Wh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8.**What is sklearn.preprocessing?\n",
        "\n",
        "**Ans:-** In the context of scikit-learn (sklearn), preprocessing refers to the techniques used to transform raw data into a format suitable for machine learning algorithms, improving model performance and ensuring accurate results.\n"
      ],
      "metadata": {
        "id": "mS492X_g0GCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.**What is a Test set?\n",
        "\n",
        "**Ans:-** \"test set\" is a subset of data used to evaluate the performance of a trained model or system, ensuring its ability to generalize to unseen data."
      ],
      "metadata": {
        "id": "jSGL9_O80545"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?"
      ],
      "metadata": {
        "id": "OII3GUnE1dA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11.**Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "**Ans:-** Performing EDA (Exploratory Data Analysis) before fitting a model is crucial because it helps you understand the data's structure, identify patterns, and detect anomalies, which informs data cleaning, feature selection, and ultimately leads to more effective model building.\n",
        "Here's a more detailed explanation:\n",
        "\n",
        "\n",
        "Understanding the Data:\n",
        "\n",
        "EDA allows you to visualize and summarize the key characteristics of your dataset, including distributions, relationships between variables, and potential outliers.\n",
        "\n",
        "Identifying Anomalies and Errors:\n",
        "\n",
        "EDA helps uncover inconsistencies, missing values, and data quality issues that could negatively impact model performance.\n",
        "Informing Data Cleaning and Preprocessing:\n",
        "By understanding the data's characteristics, you can make informed decisions about how to handle missing values, outliers, and other data quality issues.\n",
        "\n",
        "Facilitating Feature Selection:\n",
        "\n",
        "EDA can reveal which features are most important for predicting the target variable, helping you choose the right features for your model.\n",
        "Choosing the Right Model:\n",
        "EDA helps you understand the nature of the data and the relationships between variables, which can inform the choice of the most appropriate model for your task.\n",
        "\n",
        "Improving Model Performance:\n",
        "\n",
        "By addressing data quality issues and selecting the right features, EDA can lead to more accurate and reliable models.\n",
        "Detecting Hidden Patterns:\n",
        "EDA can reveal hidden patterns and relationships within the data that might not be immediately obvious, which can lead to new insights and better model performance."
      ],
      "metadata": {
        "id": "UlqWSXNr1olI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12.**What is correlation?\n",
        "\n",
        "\n",
        "**Ans:-** refered  Q2 ans"
      ],
      "metadata": {
        "id": "Dl1vYWhm2FdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.What does negative correlation mean?\n",
        "\n",
        "**Ans:-** refered  Q2 ans"
      ],
      "metadata": {
        "id": "xUZ6pffP2F1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14.** How can you find correlation between variables in Python?\n",
        "\n",
        "** Ans:** To calculate correlation in Python, you can use the pandas library, specifically the corr() method for DataFrames. It returns a correlation matrix showing the relationships."
      ],
      "metadata": {
        "id": "GGTW2CUw2F4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15.**What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "**Ans:** Causation means one event directly leads to another, while correlation simply indicates a relationship between two events, without implying a causal link. For example, ice cream sales and pool drownings are correlated during summer, but eating ice cream does not cause drowning.\n",
        "\n",
        "Causation:\n",
        "One event or variable directly influences or produces another. It implies a cause-and-effect relationship.\n",
        "\n",
        "Correlation:\n",
        "Two events or variables tend to occur together, but one doesn't necessarily cause the other. It simply indicates a relationship or association.\n",
        "\n",
        "Example:\n",
        "\n",
        "Correlation:\n",
        "The number of ice cream sales and the number of pool drownings both increase during the summer months.\n",
        "\n",
        "Causation:\n",
        "If you turn on a light switch, the light turns on. The switch action directly causes the light to illuminate."
      ],
      "metadata": {
        "id": "Q_9rX0Qx3n6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**16.**What is an Optimizer? What are different types of optimizers? Explain each with an example\n",
        "\n",
        "**Ans:**In the context of machine learning, an optimizer is an algorithm that adjusts model parameters (like weights and biases) to minimize a loss function during training, thereby improving model accuracy and convergence.\n",
        "\n",
        "Types of Optimizers and Examples:\n",
        "\n",
        "Gradient Descent (GD):\n",
        "\n",
        "This is a foundational optimization algorithm that iteratively adjusts parameters in the direction of the negative gradient of the loss function.\n",
        "\n",
        "Example: Imagine you're trying to roll a ball down a hill (the loss function). Gradient descent would calculate the slope (gradient) at your current position and move the ball in the direction of the steepest descent.\n",
        "\n",
        "Stochastic Gradient Descent (SGD):\n",
        "\n",
        "An extension of GD that updates parameters after processing each training example (or a small batch) instead of waiting for the entire dataset.\n",
        "\n",
        "Example: In a large dataset, SGD would update the ball's position after each pebble it rolls over, instead of waiting to see the entire hill.\n",
        "\n",
        "Momentum:\n",
        "\n",
        "This technique adds a \"momentum\" term to the parameter updates, which helps the optimization process to escape local minima and converge faster.\n",
        "\n",
        "Example: Imagine the ball gaining speed as it rolls down the hill, carrying it over small bumps and towards the bottom.\n",
        "\n",
        "Adagrad (Adaptive Gradient Descent):\n",
        "\n",
        "Dynamically adjusts the learning rate for each parameter based on the historical gradients, allowing for more efficient optimization.\n",
        "\n",
        "Example: If a parameter is frequently updated, Adagrad would reduce its learning rate, while parameters that are updated less often would have a higher learning rate.\n",
        "\n",
        "Adam (Adaptive Moment Estimation):\n",
        "\n",
        "Combines the benefits of both momentum and Adagrad, using adaptive learning rates and momentum to achieve faster and more stable convergence.\n",
        "\n",
        "Example: Adam would use the momentum to overcome small bumps and the adaptive learning rate to adjust the speed of the ball based on the terrain.\n",
        "\n",
        "RMSprop (Root Mean Squared Propagation):\n",
        "\n",
        "Another adaptive learning rate method that uses the root mean square of the gradients to adjust the learning rate.\n",
        "\n",
        "Example: RMSprop would use the average of the squared gradients to adjust the speed of the ball, similar to how a car would adjust its speed based on the average speed of the road.\n",
        "\n",
        "Mini-batch Gradient Descent:\n",
        "\n",
        "A compromise between SGD and Batch Gradient Descent, where the gradient is computed over a randomly selected subset of the training data, typically called a mini-batch.\n",
        "\n",
        "Example: Instead of updating the ball's position after each pebble (SGD) or waiting for the entire hill (Batch GD), mini-batch GD would update the position after rolling over a small section of the hill.\n"
      ],
      "metadata": {
        "id": "bUqxRniI4Hgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**17.**What is sklearn.linear_model ?\n",
        "\n",
        "**Ans:** linear_model is a class of the sklearn module if contain different functions for performing machine learning with linear models. The term linear model implies that the model is specified as a linear combination of features"
      ],
      "metadata": {
        "id": "lNLVNmxK45wp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**18.**What does model.fit() do? What arguments must be given?\n",
        "\n",
        "**Ans:** In TensorFlow,model. fit() function is used to train a machine learning model for a fixed number of epochs (iterations over the entire dataset). During training, the model adjusts its internal parameters (weights and biases) to minimize the loss function using optimization techniques like Gradient Descent"
      ],
      "metadata": {
        "id": "Pp4TT1jx5P9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**19**.What does model.predict() do? What arguments must be given?\n",
        "\n",
        "**Ans:**Purpose : model. predict() is used to generate predictions from the trained model based on new input data. It does not require true labels and does not compute any metrics."
      ],
      "metadata": {
        "id": "2Shc3HN45v8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**20.**What are continuous and categorical variables?\n",
        "\n",
        "refered Q5 ans"
      ],
      "metadata": {
        "id": "LldU89_P6MYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**21.**What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "**Ans:**\n",
        "Feature scaling is a data preprocessing technique that transforms the values of numerical features to a common scale, ensuring that all features contribute equally to the machine learning model's training and performance.\n",
        "\n",
        "Here's a more detailed explanation:\n",
        "\n",
        "Why is Feature Scaling Important?\n",
        "\n",
        "Equal Contribution:\n",
        "Machine learning algorithms treat all features as equally important unless otherwise specified. Without scaling, features with larger ranges might dominate the model's learning process, leading to biased results.\n",
        "Improved Algorithm Performance:\n",
        "Many algorithms, especially those based on distance or gradient descent, perform better when features are on a similar scale.\n",
        "\n",
        "Faster Convergence:\n",
        "Scaling can help algorithms converge faster and more efficiently during training, saving time and computational resources.\n",
        "Avoiding Numerical Instability:\n",
        "Significant differences in feature scales can lead to numerical instability, especially in algorithms that involve distance calculations or gradient descent.\n",
        "\n",
        "Better Visualization:\n",
        "Scaling can also make it easier to visualize and interpret the data, as all features are on a comparable scale.\n",
        "\n",
        "Common Feature Scaling Methods:\n",
        "\n",
        "Normalization:\n",
        "Transforms values to a specific range, often between 0 and 1, using techniques like Min-Max Scaling.\n",
        "\n",
        "Standardization:\n",
        "Transforms features so they have a mean of 0 and a standard deviation of 1, also known as Z-score normalization.\n",
        "\n",
        "Robust Scaling:\n",
        "Uses percentiles instead of mean and standard deviation, making it more resistant to outliers.\n",
        "\n",
        "Logarithmic Transformation:\n",
        "Useful for data with a wide range of values, compressing the scale and making it more uniform.\n",
        "\n",
        "When to Use Feature Scaling:\n",
        "Algorithms that rely on distance or gradient calculations: k-Nearest Neighbors, Support Vector Machines, and Neural Networks are examples.\n",
        "\n",
        "Algorithms that are sensitive to feature scales: Linear Regression and Logistic Regression.\n",
        "\n",
        "When features have very different ranges: For example, age (0-100) and income (thousands or millions).\n",
        "When you want to ensure all features contribute equally to the model's training:"
      ],
      "metadata": {
        "id": "Mz0i36qR6WTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**22.**How do we perform scaling in Python?\n",
        "\n",
        "**Ans:** Two primary methods for scaling are a standard scaler (scale by the standard deviation) and a min-max (e.g. 0-1) scaler. For classifiers and regressor such as neural networks, most of the data should be between 0 and 1 or -1 and 1. The scaled output is y and the unscaled input is x."
      ],
      "metadata": {
        "id": "9dvpcic669Ex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**23.**What is sklearn.preprocessing?\n",
        "\n",
        "refered Q5 ans"
      ],
      "metadata": {
        "id": "-3Oi1UsJ7WvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**24.**How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "refered Q5 ans"
      ],
      "metadata": {
        "id": "lNsEzZPu7fuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**25.**Explain data encoding?\n",
        "\n",
        "**Ans:**Data Encoding refers to the converting of categorical variables into numerical representations that can be understood by machine learning algorithms"
      ],
      "metadata": {
        "id": "_nQK3lMq7pEu"
      }
    }
  ]
}